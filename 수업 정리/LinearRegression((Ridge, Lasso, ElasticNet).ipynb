{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-fold Cross Validation(K-fold 교차 검증)\n",
    "- 데이터의 개수가 너무 작을 경우, 트레이닝 데이터와 테스트 데이터가 어떻게 나눠지는가에 따라 학습된 모델과 성능 측정 결과가 크게 달라질 수 있음.\n",
    "- 따라서 이런 문제를 해결하기 위해 K-Fold Validation 을 사용\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "X=np.array([[1,2],[3,4],[1,2],[3,4]])\n",
    "y=np.array([1,2,3,4])\n",
    "kf=KFold(n_splits=2)\n",
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=2, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(train_index,test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "- Feature Engineering 은 도메인 지식이나 분석을 통해서 유의미한 특징들만을 선별해내거나 Featue의 형태를 더욱 적합한 형태로 변경하는 것\n",
    "\n",
    "- 적절한 feature engineering 머신러닝 모델의 성능에 큰 영향을 미침\n",
    "\n",
    "크게 3가지로 나눔\n",
    "1. Feature Selection\n",
    "- 예측값과 연관이 없는 불필요한 특징을 제거해서 머신러닝 모델의 성능을 더욱 놓이는 기법\n",
    "- 제거할 특징을 선택하기 위해 상관 분석 등을 진행\n",
    "\n",
    "2. Normalization\n",
    "- 도메인 지식이나 분석을 통해서 유의미한 특징들만을 선별해내거나 Feature 의 형태를 더욱 적합한 형태로 변경하는 것을 뜻\n",
    "- 특정 값의 범위를 조정하는 것\n",
    "- Featrue 정규화 - Standardization\n",
    "-> Feature 를 정규화할 경우, 값의 분포를 정규분포 형태로 변경\n",
    "\n",
    "-> 일반적으로 feature 값에 대한 정규화를 수행할 경우, 더 안정적으로 머신러닝 모델을 학습시킬 수 있음\n",
    "\n",
    "- Featrue 정규화 - Min-Max Scaling\n",
    "-> Feature 를 Min-Max Scaling할 경우, 값을 0~1 사이의 범위로 변경\n",
    "\n",
    "\n",
    "3. Feature Generation\n",
    "- 특정값들을 조합해서 새로운 특징을 만들어냄\n",
    "- PolynomialFeatures 를 이용할 경우, 서로 다른 feature 들 간의 곱셈을 새로운 feature 로 만들 수 있음\n",
    "- 즉, 예를 들어 CRIM(범죄율)x1 이라는 특징과 LSTAT(저소득측 비율) x2 2개으 특징을 곱해서 CRIM*LSTAT이라는 새로운 특징 추출\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상관분석 (Correlation Analysis)\n",
    "\n",
    "- 상관 분석 또는 '상관관계' 또는 분석하는 '상관' 은 확률론과 통계학에서 두 변수 간에 어떤 선형적 또는 비선형적 관계를 갖고 있는 지를 판단하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1에 가까운 값 : 두 변수들 간의 양의 상관 관계가 있음\n",
    "- 0에 가까운 값 : 두 변수들 간의 상관관계가 없음\n",
    "- -1 에 가까운 값 : 두 변수들 간의 음의 상관관계가 있음\n",
    "\n",
    "-sns.regplot()으로 Feature들 간의 경향성 출력하기\n",
    "\n",
    "sns.regplot(data={dataframe},x={x컬럼명},y={y컬럼명}) 형태를 이용해서 regression line 이 포함된 scatter plot 을 그릴 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "- 기본 Linear Regression 은 input data의 feature X에 가중치 W를 곱해서 적절한 y 값을 예측하는 것\n",
    "- 이때 학습을 통해 적절한 가중치 w를 찾아야 함\n",
    "- 기본 Linear Regression 은 train data에 과도하게 최적화된 overfitting 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting , Underfitting\n",
    "- 오버피팅은 학습 과정에서 머신러닝 알고리즘의 파라미터가 train data에 과도하게 최적화된 것으로 새로운 데이터를 넣었을 때 제대로 예측하지 못하는 것을 뜻함.\n",
    "- 언더 피팅은 모델의 표현력이 부족해 train data도 제대로 예측하지 못하는 것을 뜻함.\n",
    "- 오버피팅을 방지하기 위한 기법들을 Regularization 기법이라 함.\n",
    "![Alt text](image-7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Polynomial Features 적용과 기본 Linear Regression 의 Overfitting 문제 발생 -> 차원이 높아지면서 오버피팅에 빠질 가능성 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 기본 Linear Regression 확장 알고리즘\n",
    "- 기본 Linear Regrssion : Regularization을 적용하지 않은 알고리즘\n",
    "- Ridge : L2 Regularization 을 적용한 모델\n",
    "- Lasso : L1 Regularization 을 적용한 모델\n",
    "- ElasticNet : L1 Regularization 과 L2 Regularization 을 함께 적용한 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 릿지 회귀(Ridge Regression)\n",
    "![Alt text](image-9.png)\n",
    "\n",
    "- 모델 성능을 높이는데 기여하지 못하는 독립변수의 회귀계수 크기를 0에 근접하도록 축소 시킴( 0에 근사할 뿐, 0이 되지는 않는다.)\n",
    "- L2- norm 이라고 부르는 페널티항을 통해 페널티 부과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 라쏘 회귀(Lasso Regression)\n",
    "![Alt text](image-10.png)\n",
    "- 모델 성능을 높이지 못하는 독립변수의 회귀계수를 0으로 만드는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 일래스틱 회귀(ElasticNet)\n",
    "![Alt text](image-11.png)\n",
    "- 릿지와 라쏘의 결합\n",
    "- 일부 독립변수의 회귀계수 크기를 축고할 수도 있고, 0으로 만들수 도 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
